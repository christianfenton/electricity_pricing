{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3510acc",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "\n",
    "**New variables:**\n",
    "\n",
    "Demand:\n",
    "- [x] Actual values and generation mix \n",
    "    (https://www.neso.energy/data-portal/historic-demand-data)\n",
    "- [x] Day-ahead forecasts \n",
    "    (https://www.neso.energy/data-portal/1-day-ahead-demand-forecast/historic_day_ahead_demand_forecasts)\n",
    "\n",
    "Generation:\n",
    "- [x] Historic generation mix \n",
    "    (https://www.neso.energy/data-portal/historic-generation-mix)\n",
    "- [x] Day-ahead metered wind forecasts\n",
    "    (https://www.neso.energy/data-portal/day-ahead-wind-forecast)\n",
    "- [x] Embedded wind and solar forecasts \n",
    "    (https://www.neso.energy/data-portal/embedded-wind-and-solar-forecasts)\n",
    "\n",
    "Prices:\n",
    "- [x] Electricity prices (BMRS MIP)\n",
    "- [x] Natural gas prices (ONS, already sourced)\n",
    "- [ ] Coal prices\n",
    "- [ ] Carbon credit prices\n",
    "\n",
    "*Note: Prices of futures contracts would be preferable, \n",
    "but these are often behind paywalls.*\n",
    "\n",
    "Temporal features:\n",
    "- [x] `holidays` package\n",
    "\n",
    "(Optional) Capacity:\n",
    "- Nuclear capacity\n",
    "- Gas capacity\n",
    "- Interconnector capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de6464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990f4f41",
   "metadata": {},
   "source": [
    "## Historical Actual Demand Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaab5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2020-12-31\"\n",
    "end_date = \"2025-01-02\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710cf188",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.neso.energy/api/3/action/datastore_search_sql\"\n",
    "resource_id = \"b2bde559-3455-4021-b179-dfe60c0337b0\"\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM \"{resource_id}\"\n",
    "WHERE \"SETTLEMENT_DATE\" >= '{start_date}' AND \"SETTLEMENT_DATE\" <= '{end_date}'\n",
    "ORDER BY \"SETTLEMENT_DATE\" ASC\n",
    "\"\"\"\n",
    "\n",
    "params = {\"sql\": query}\n",
    "\n",
    "r = requests.get(url, params=params)\n",
    "if r.status_code == 200:\n",
    "    demand_actual = r.json()\n",
    "else:\n",
    "    raise Exception(\"Request Failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e718011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack data into a dataframe\n",
    "# note: ND = national demand, TSD = transition system demand\n",
    "df_demand_actual = pd.DataFrame(demand_actual['result']['records'])\n",
    "df_demand_actual.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b2ad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand_actual['SETTLEMENT_DATE'] = pd.to_datetime(df_demand_actual['SETTLEMENT_DATE'])\n",
    "\n",
    "# keep only required columns\n",
    "df_demand_actual = df_demand_actual[['ND', 'TSD', 'SETTLEMENT_DATE', 'SETTLEMENT_PERIOD']]\n",
    "\n",
    "\n",
    "df_demand_actual.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ed99d7",
   "metadata": {},
   "source": [
    "## Historical Generation Mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84b6b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.neso.energy/api/3/action/datastore_search_sql\"\n",
    "resource_id = \"f93d1835-75bc-43e5-84ad-12472b180a98\"\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM \"{resource_id}\"\n",
    "WHERE \"DATETIME\" >= '{start_date}' AND \"DATETIME\" <= '{end_date}'\n",
    "ORDER BY \"DATETIME\" ASC\n",
    "\"\"\"\n",
    "\n",
    "params = {\"sql\": query}\n",
    "\n",
    "r = requests.get(url, params=params)\n",
    "if r.status_code == 200:\n",
    "    generation_actual = r.json()\n",
    "else:\n",
    "    raise Exception(\"Request Failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e4f9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack data into a dataframe\n",
    "df_generation_actual = pd.DataFrame(generation_actual['result']['records'])\n",
    "df_generation_actual.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec793b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set timestamp as index\n",
    "df_generation_actual['DATETIME'] = pd.to_datetime(df_generation_actual['DATETIME'])\n",
    "df_generation_actual.set_index('DATETIME', inplace=True)\n",
    "\n",
    "# drop unnecessary columns\n",
    "df_generation_actual.drop(columns=['_id', '_full_text', 'FOSSIL', 'RENEWABLE', 'ZERO_CARBON', 'LOW_CARBON', 'CARBON_INTENSITY'], inplace=True)\n",
    "df_generation_actual.drop(columns=list(df_generation_actual.filter(regex='_perc')), inplace=True)\n",
    "\n",
    "# fix non-numeric columns\n",
    "df_generation_actual = df_generation_actual.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# check data is correct\n",
    "calculated_generation = df_generation_actual.drop(columns='GENERATION').sum(axis=1)\n",
    "relative_error = (df_generation_actual['GENERATION'] - calculated_generation.abs()) / calculated_generation.abs()\n",
    "assert np.allclose(relative_error, 0, atol=1e-3)\n",
    "\n",
    "df_generation_actual.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b84fe0",
   "metadata": {},
   "source": [
    "## Historical Day-Ahead Demand Forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0a6d1f",
   "metadata": {},
   "source": [
    "*Definitions:*\n",
    "\n",
    "Cardinal Point: Electricity demand fluctuates during a day depending on \n",
    "how much energy people, businesses and industries are using at that \n",
    "moment in time. As this electricity demand goes up and down we get \n",
    "characteristic peaks and troughs, with some of these peaks and troughs \n",
    "appearing every single day at similar times. These we call cardinal \n",
    "points and are the points during the day that we forecast demand for.\n",
    "\n",
    "Cardinal Point Type: Fixed, Trough or Peak. Cardinal points (CPs) can \n",
    "either be fixed (occur at a fixed time), trough (minimum demands during \n",
    "a set period of the day) or peak(maximum demands during a set period of \n",
    "the day). These are represented throught the first letter of the point \n",
    "type (F,T or P)\n",
    "\n",
    "Cardinal Point Start Time: The time when a particular cardinal point \n",
    "(CP) starts during the day. This is given relative to the timezone in \n",
    "effect in the UK at the forecast time and date.\n",
    "\n",
    "Cardinal Point End Time: The time when a particular cardinal point (CP) \n",
    "ends during the day. This is given relative to the timezone in effect \n",
    "in the UK at the forecast time and date.\n",
    "\n",
    "Forecasting Point: Forecasts of forecasting points\n",
    "- Forecasting point 1 / Overnight minimum (OM): minimum national demand \n",
    "    between half hour ending 00:30 and 07:30. \n",
    "- Forecasting point 2 / Daytime peak (DM): maximum national demand \n",
    "    between half hour ending 08:00 and 13:00. \n",
    "- Forecasting point 3 / Daytime minimum (Dm): minimum national demand \n",
    "    between half hour ending 13:30 and 16:30. \n",
    "- Forecasting point 4 / Evening peak (EM): maximum national demand \n",
    "    between half hour ending 17:00 and 24:00. \n",
    "\n",
    "Forecast Timestamp: The date and time at which the forecast was made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ee7b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.neso.energy/api/3/action/datastore_search_sql\"\n",
    "resource_id = \"9847e7bb-986e-49be-8138-717b25933fbb\"\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM \"{resource_id}\"\n",
    "WHERE \"TARGETDATE\" >= '{start_date}' AND \"TARGETDATE\" <= '{end_date}'\n",
    "ORDER BY \"TARGETDATE\" ASC\n",
    "\"\"\"\n",
    "\n",
    "params = {\"sql\": query}\n",
    "\n",
    "r = requests.get(url, params=params)\n",
    "if r.status_code == 200:\n",
    "    demand_forecast = r.json()\n",
    "else:\n",
    "    raise Exception(\"Request Failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d27c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack data into a dataframe\n",
    "df_demand_dayahead = pd.DataFrame(demand_forecast['result']['records'])\n",
    "df_demand_dayahead.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b769083",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand_dayahead.drop(columns=['_full_text', '_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71fb918",
   "metadata": {},
   "source": [
    "Convert time data to a more usable format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dc7687",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_demand_dayahead.copy()\n",
    "\n",
    "# time strings padded with zeros to have standard format\n",
    "cp_st_time_str = df['CP_ST_TIME'].astype(str).str.zfill(4)\n",
    "cp_end_time_str = df['CP_END_TIME'].astype(str).str.zfill(4)\n",
    "\n",
    "# identify rows with 2400 (these need to be converted to 0000)\n",
    "cp_st_mask = df['CP_ST_TIME'] == '2400'\n",
    "cp_end_mask = df['CP_END_TIME'] == '2400'\n",
    "\n",
    "# replace 2400 with 0000\n",
    "cp_st_time_str = cp_st_time_str.replace('2400', '0000')\n",
    "cp_end_time_str = cp_end_time_str.replace('2400', '0000')\n",
    "\n",
    "# combine date and time\n",
    "df['CP_START'] = pd.to_datetime(df['TARGETDATE'].astype(str) + ' ' + cp_st_time_str, format='%Y-%m-%d %H%M')\n",
    "df['CP_END'] = pd.to_datetime(df['TARGETDATE'].astype(str) + ' ' + cp_end_time_str, format='%Y-%m-%d %H%M')\n",
    "\n",
    "# localise to Europe/London — handle DST transitions\n",
    "df['CP_START'] = df['CP_START'].dt.tz_localize('Europe/London', ambiguous='NaT', nonexistent='NaT')\n",
    "df['CP_END'] = df['CP_END'].dt.tz_localize('Europe/London', ambiguous='NaT', nonexistent='NaT')\n",
    "\n",
    "# drop rows where timestamps couldn’t be localized (spring-forward gap)\n",
    "# df = df.dropna(subset=['CP_START', 'CP_END'])\n",
    "\n",
    "# add one day where the original time was 2400\n",
    "df.loc[cp_st_mask, 'CP_START'] += pd.Timedelta(days=1)\n",
    "df.loc[cp_end_mask, 'CP_END'] += pd.Timedelta(days=1)\n",
    "\n",
    "# drop unnecessary columns\n",
    "df.drop(columns=['CP_ST_TIME', 'CP_END_TIME', 'TARGETDATE'], inplace=True)\n",
    "if (df_demand_dayahead['DAYSAHEAD'] == 1).all():\n",
    "    df.drop(columns=['DAYSAHEAD'], inplace=True)\n",
    "else:\n",
    "    raise Exception(\"Expected day-ahead forecast.\")\n",
    "\n",
    "df_demand_dayahead = df\n",
    "\n",
    "df_demand_dayahead.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c89be13",
   "metadata": {},
   "source": [
    "*Definitions:*\n",
    "\n",
    "**Cardinal Point:** Electricity demand fluctuates during a day depending\n",
    "on how much energy people, businesses and industries are using at that\n",
    "moment in time. As this electricity demand goes up and down we get\n",
    "characteristic peaks and troughs, with some of these peaks and troughs\n",
    "appearing every single day at similar times. These we call cardinal\n",
    "points and are the points during the day that we forecast demand for.\n",
    "\n",
    "**Cardinal Point Type:** Fixed, Trough or Peak. Cardinal points (CPs) \n",
    "can either be fixed (occur at a fixed time), trough (minimum demands \n",
    "during a set period of the day) or peak(maximum demands during a set \n",
    "period of the day). These are represented throught the first letter of \n",
    "the point type (F,T or P)\n",
    "\n",
    "**Cardinal Point Start Time:** The time when a particular cardinal point \n",
    "(CP) starts during the day. This is given relative to the timezone in \n",
    "effect in the UK at the forecast time and date.\n",
    "\n",
    "**Cardinal Point End Time:** The time when a particular cardinal point \n",
    "(CP) ends during the day. This is given relative to the timezone in \n",
    "effect in the UK at the forecast time and date.\n",
    "\n",
    "**Forecasting Point:** Forecasts of forecasting points\n",
    "- Forecasting point 1 / Overnight minimum (OM): minimum national demand \n",
    "    between half hour ending 00:30 and 07:30. \n",
    "- Forecasting point 2 / Daytime peak (DM): maximum national demand \n",
    "    between half hour ending 08:00 and 13:00. \n",
    "- Forecasting point 3 / Daytime minimum (Dm): minimum national demand\n",
    "    between half hour ending 13:30 and 16:30. \n",
    "- Forecasting point 4 / Evening peak (EM): maximum national demand \n",
    "    between half hour ending 17:00 and 24:00. \n",
    "\n",
    "**Forecast Timestamp:** The date and time at which the forecast was made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e08a95",
   "metadata": {},
   "source": [
    "Move times to a uniformly spaced grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc0ab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_demand_dayahead.copy()\n",
    "\n",
    "# Assume cardinal point is at the midpoint of the start and end times\n",
    "df['CP_MIDPOINT'] = df['CP_START'] + (df['CP_END'] - df['CP_START']) / 2\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df.drop(columns=['CP_TYPE', 'F_Point', 'CARDINALPOINT', 'CP_START', 'CP_END'], inplace=True)\n",
    "\n",
    "# Create a uniformly spaced 30-minute index in Europe/London time\n",
    "df.set_index('CP_MIDPOINT', inplace=True)\n",
    "start = df.index.min().floor('30min')\n",
    "end = df.index.max().ceil('30min')\n",
    "uniform_index = pd.date_range(start=start, end=end, freq='30min', tz='Europe/London')\n",
    "\n",
    "# drop duplicate indices (happens when forecast gets updated)\n",
    "df = df.loc[~df.index.duplicated(keep='first'), :]  # keep first forecast\n",
    "\n",
    "# reindex the data to get uniform spacing\n",
    "df = df.reindex(uniform_index, fill_value=np.nan)\n",
    "\n",
    "# interpolate\n",
    "df['FORECASTDEMAND'] = df['FORECASTDEMAND'].interpolate(method='linear')\n",
    "\n",
    "# Add settlement date and settlement period\n",
    "# Settlement periods are 1–48 per day, each 30 minutes long\n",
    "df['SETTLEMENT_DATE'] = df.index.tz_convert('Europe/London').date\n",
    "df = df.sort_index()\n",
    "df['SETTLEMENT_PERIOD'] = df.groupby('SETTLEMENT_DATE').cumcount() + 1\n",
    "\n",
    "# Reset index\n",
    "df = df.reset_index().rename(columns={'index': 'DATETIME'})\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df.drop(columns=['DATETIME', 'FORECAST_TIMESTAMP'], inplace=True)\n",
    "\n",
    "# rename columns\n",
    "df.rename(columns={'FORECASTDEMAND': 'DEMAND_FORECAST'}, inplace=True)\n",
    "\n",
    "df_demand_dayahead = df.copy()\n",
    "\n",
    "# display\n",
    "df_demand_dayahead.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78daf836",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand_dayahead.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8de7ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS:\n",
    "\n",
    "import datetime\n",
    "\n",
    "def keep_dates(\n",
    "        df: pd.DataFrame, \n",
    "        date_range = (datetime.date(2021, 1, 1), datetime.date(2025, 1, 1)), \n",
    "        date_column: str = 'SETTLEMENT_DATE'):\n",
    "    dates = df[date_column]\n",
    "    df = df[(date_range[0] <= dates) & (dates < date_range[1])]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def last_sunday_of_month(year, month):\n",
    "    \"\"\"Find the last Sunday of a given month\"\"\"\n",
    "    last_day = pd.Timestamp(year=year, month=month, day=1) + pd.offsets.MonthEnd(1)\n",
    "    days_back = (last_day.dayofweek - 6) % 7  # Sunday is 6\n",
    "    last_sunday = last_day - pd.Timedelta(days=days_back)\n",
    "    return last_sunday.date()\n",
    "\n",
    "def check_day_lengths(df: pd.DataFrame, date_column='SETTLEMENT_DATE'):\n",
    "    \"\"\"Check days have expected number of settlement periods.\"\"\"\n",
    "    daily_counts = df.groupby(df[date_column]).size()  # count settlement periods per day\n",
    "    irregular_days = daily_counts[(daily_counts != 48) & (daily_counts != 46) & (daily_counts != 50)]\n",
    "    assert len(irregular_days) == 0, \"Found days with an irregular number of settlement periods.\"\n",
    "    return True\n",
    "\n",
    "def check_spring_dst(df: pd.DataFrame, date_column='SETTLEMENT_DATE'):\n",
    "    daily_counts = df.groupby(df[date_column]).size()  # count settlement periods per day\n",
    "    short_days = daily_counts[daily_counts == 46].index\n",
    "    short_days = sorted(short_days)\n",
    "    years_in_data = df[date_column].dt.year.unique()\n",
    "    years = sorted(years_in_data)\n",
    "    assert len(years) == len(short_days), \"Expected only one short day per year.\"\n",
    "    for (i, year) in enumerate(years):\n",
    "        expected_date = last_sunday_of_month(year, 3)\n",
    "        short_day = short_days[i].date()\n",
    "        error_msg = f\"Incorrect short day date in year {year}.\\nExpected {expected_date}, got {short_day}.\"\n",
    "        assert short_day == expected_date, error_msg\n",
    "    return True\n",
    "\n",
    "def check_autumn_dst(df: pd.DataFrame, date_column='SETTLEMENT_DATE'):\n",
    "    daily_counts = df.groupby(df[date_column]).size()  # count settlement periods per day\n",
    "    long_days = daily_counts[daily_counts == 50].index\n",
    "    long_days = sorted(long_days)\n",
    "    years_in_data = df[date_column].dt.year.unique()\n",
    "    years = sorted(years_in_data)\n",
    "    assert len(years) == len(long_days), \"Expected only one short day per year.\"\n",
    "    for (i, year) in enumerate(years):\n",
    "        expected_date = last_sunday_of_month(year, 10)\n",
    "        long_day = long_days[i].date()\n",
    "        error_msg = f\"Incorrect short day date in year {year}.\\nExpected {expected_date}, got {long_day}.\"\n",
    "        assert long_day == expected_date, error_msg\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dafa335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows outside 2021-2024 date range\n",
    "date_range = (datetime.date(2021, 1, 2), datetime.date(2025, 1, 1))\n",
    "df_demand_dayahead = keep_dates(df_demand_dayahead, date_range=date_range, date_column='SETTLEMENT_DATE')\n",
    "\n",
    "# check that number of rows is as expected\n",
    "df = df_demand_dayahead.copy()\n",
    "df['SETTLEMENT_DATE'] = pd.to_datetime(df['SETTLEMENT_DATE'])\n",
    "assert check_day_lengths(df)\n",
    "assert check_spring_dst(df)\n",
    "assert check_autumn_dst(df)\n",
    "\n",
    "df_demand_dayahead.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f75a8e",
   "metadata": {},
   "source": [
    "## Wind Generation Forecasts\n",
    "\n",
    "These are forecasts for metered wind generation, i.e. not including embedded systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aab7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.neso.energy/api/3/action/datastore_search_sql\"\n",
    "resource_id = \"7524ec65-f782-4258-aaf8-5b926c17b966\"\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM \"{resource_id}\"\n",
    "WHERE \"Datetime_GMT\" >= '{start_date}' AND \"Datetime_GMT\" <= '{end_date}'\n",
    "ORDER BY \"Datetime_GMT\" ASC\n",
    "\"\"\"\n",
    "\n",
    "params = {\"sql\": query}\n",
    "\n",
    "r = requests.get(url, params=params)\n",
    "if r.status_code == 200:\n",
    "    wind_forecast = r.json()\n",
    "else:\n",
    "    raise Exception(\"Request Failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e42c9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wind_dayahead = pd.DataFrame(wind_forecast['result']['records'])\n",
    "df_wind_dayahead.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7645779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "df_wind_dayahead.drop(columns=['_full_text', '_id', 'Forecast_Timestamp', 'Datetime_GMT'], inplace=True)\n",
    "\n",
    "# rename to match conventions of other dataframes\n",
    "df_wind_dayahead.rename(\n",
    "    columns={\n",
    "        'Incentive_forecast': 'WIND_FORECAST',\n",
    "        'Settlement_period': 'SETTLEMENT_PERIOD',\n",
    "        'Date': 'SETTLEMENT_DATE',\n",
    "        'Capacity': 'WIND_CAPACITY'\n",
    "        }, \n",
    "    inplace=True\n",
    "    )\n",
    "\n",
    "# convert strings to datetime\n",
    "df_wind_dayahead['SETTLEMENT_DATE'] = pd.to_datetime(df_wind_dayahead['SETTLEMENT_DATE']).dt.date\n",
    "\n",
    "df_wind_dayahead.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c62d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wind_dayahead.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636d39b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows outside 2021-2024 date range\n",
    "date_range = (datetime.date(2021, 1, 2), datetime.date(2025, 1, 1))\n",
    "df_wind_dayahead = keep_dates(df_wind_dayahead, date_range=date_range, date_column='SETTLEMENT_DATE')\n",
    "\n",
    "# check that number of rows is as expected\n",
    "df = df_wind_dayahead.copy()\n",
    "df['SETTLEMENT_DATE'] = pd.to_datetime(df['SETTLEMENT_DATE'])\n",
    "assert check_day_lengths(df)\n",
    "assert check_spring_dst(df)\n",
    "assert check_autumn_dst(df)\n",
    "\n",
    "df_wind_dayahead.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4a7000",
   "metadata": {},
   "source": [
    "## Embedded Wind and Solar Forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387bbba2",
   "metadata": {},
   "source": [
    "Load from CSVs available on NESO website since the files are large which makes API calls very slow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5950303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"raw\", \"neso\")\n",
    "years = [2021, 2022, 2023, 2024]\n",
    "dfs = []\n",
    "\n",
    "for year in years:\n",
    "    load_path = os.path.join(base_dir, f\"embedded_archive_{year}.csv\")\n",
    "    df = pd.read_csv(load_path)  # file size is 300+ MB\n",
    "\n",
    "    # combine separate date and time columns + convert other timestamp columns to correct format\n",
    "    dates_str = (pd.to_datetime(df['DATE_GMT']).dt.date).astype(\"string\")\n",
    "    df['DATETIME_GMT'] = pd.to_datetime(dates_str + 'T' + df['TIME_GMT'], format='ISO8601', utc=True)\n",
    "    df.drop(columns=['DATE_GMT', 'TIME_GMT'], inplace=True)\n",
    "    df['Forecast_Datetime'] = pd.to_datetime(df['Forecast_Datetime'])\n",
    "    df['SETTLEMENT_DATE'] = pd.to_datetime(df['SETTLEMENT_DATE']).dt.date\n",
    "\n",
    "    # keep last forecast made on the previous day before 09:00 (market close is at 10:20)\n",
    "    forecast_date = df['Forecast_Datetime'].dt.date\n",
    "    target_date = df['SETTLEMENT_DATE']\n",
    "    forecast_hour = df['Forecast_Datetime'].dt.hour\n",
    "    is_previous_day = (target_date - forecast_date) == pd.Timedelta(days=1)\n",
    "    is_morning_forecast = (forecast_hour >= 0) & (forecast_hour < 9)\n",
    "    df = df[is_previous_day & is_morning_forecast]\n",
    "    \n",
    "    # remove duplicates, keeping the last forecast made before 09:00 for each DATETIME_GMT\n",
    "    df = df.sort_values('Forecast_Datetime')\n",
    "    df = df.drop_duplicates(subset='DATETIME_GMT', keep='last')\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # drop unnecessary rows to reduce memory requirements\n",
    "    df.drop(columns=['Forecast_Datetime', 'DATETIME_GMT'], inplace=True)\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "df_emb_dayahead = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(df_emb_dayahead.info())\n",
    "\n",
    "df_emb_dayahead.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85540e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows outside 2021-2024 date range\n",
    "date_range = (datetime.date(2021, 1, 2), datetime.date(2025, 1, 1))\n",
    "df_emb_dayahead = keep_dates(df_emb_dayahead, date_range=date_range, date_column='SETTLEMENT_DATE')\n",
    "\n",
    "# check that number of rows is as expected\n",
    "df = df_emb_dayahead.copy()\n",
    "df['SETTLEMENT_DATE'] = pd.to_datetime(df['SETTLEMENT_DATE'])\n",
    "assert check_day_lengths(df)\n",
    "assert check_spring_dst(df)\n",
    "assert check_autumn_dst(df)\n",
    "\n",
    "df_emb_dayahead.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c64b7f",
   "metadata": {},
   "source": [
    "## Merge forecast data\n",
    "\n",
    "Merge forecasts for (a) demand (b) metered wind generation, and (c) embedded wind and solar generation to form one convenient dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc2d9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_demand_dayahead, df_emb_dayahead, how='inner', on=['SETTLEMENT_DATE', 'SETTLEMENT_PERIOD'])\n",
    "df_merged = pd.merge(df_merged, df_wind_dayahead, how='inner', on=['SETTLEMENT_DATE', 'SETTLEMENT_PERIOD'])\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8598c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows outside 2021-2024 date range\n",
    "date_range = (datetime.date(2021, 1, 2), datetime.date(2025, 1, 1))\n",
    "df_merged = keep_dates(df_merged, date_range=date_range, date_column='SETTLEMENT_DATE')\n",
    "\n",
    "# check that number of rows is as expected\n",
    "df = df_merged.copy()\n",
    "df['SETTLEMENT_DATE'] = pd.to_datetime(df['SETTLEMENT_DATE'])\n",
    "assert check_day_lengths(df)\n",
    "assert check_spring_dst(df)\n",
    "assert check_autumn_dst(df)\n",
    "\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bad929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write to disk\n",
    "# save_path = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"processed\", \"forecast_data.csv\")\n",
    "# df_merged.to_csv(save_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "electricity-pricing-GKSthAoU-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
